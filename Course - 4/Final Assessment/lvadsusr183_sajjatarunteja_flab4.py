# -*- coding: utf-8 -*-
"""LVADSUSR183_SajjaTarunTeja_FLab4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GPlzBu651Nl1l8WKU8R5Vc4bywFaB__3
"""

import numpy as np
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.ensemble import IsolationForest
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

df = pd.read_csv("/content/anomaly_train.csv")

df.head()

df.info()

df.isna().sum()
# No Null Values

df.duplicated().sum()
# No Duplicates

ndf = df.drop(["TransactionID","Type","Location"],axis=1)

cdf = df[["Type","Location"]]
cdf = pd.get_dummies(cdf,columns=["Type","Location"])

cdf = cdf.applymap(lambda x: 1 if x>0 else 0)

df = pd.concat([ndf,cdf],axis=1)
df = pd.DataFrame(df)
df.head()

# Correlation heatmap
plt.figure(figsize=(8,6))
sns.heatmap(df.corr(), annot=True, cmap='summer', fmt='.2f')
plt.title('Correlation Heatmap')
plt.show()

X=df.drop(["Amount"],axis=1)
y = df["Amount"]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
model = IsolationForest(n_estimators=100, contamination=0.1, max_features=3, max_samples=10000, random_state=42)
model.fit(X_train)
y_pred = model.predict(X_train)
df["anomaly_score"] = model.decision_function(X)

anomalies = df.loc[df["anomaly_score"] < 0]

anomalies["anomaly_score"].value_counts()

plt.scatter(df["Amount"], df["anomaly_score"], label="Not Anomaly")
plt.scatter(anomalies["Amount"], anomalies["anomaly_score"], color="red", label="Anomaly")
plt.xlabel("Amount")
plt.ylabel("Anomaly Score")
plt.legend()
plt.show()

